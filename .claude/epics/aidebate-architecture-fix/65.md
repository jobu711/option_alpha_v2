---
name: Enhance health checks for both LLM backends
status: open
created: 2026-02-13T15:16:40Z
updated: 2026-02-13T15:26:14Z
github: https://github.com/jobu711/option_alpha_v2/issues/65
depends_on: [63]
parallel: true
conflicts_with: []
---

# Task: Enhance health checks for both LLM backends

## Description
Make health checks actually verify model readiness, not just API reachability. Ollama's check should confirm the model is loaded; Claude's check should validate the API key. Both should use the new `ai_health_check_timeout` setting from Task 001.

## Acceptance Criteria
- [ ] `OllamaClient.health_check()` sends a minimal `/api/generate` request (e.g., "Say OK") after the existing `/api/tags` check to verify the model is loaded and responsive
- [ ] `ClaudeClient.health_check()` sends a minimal `/v1/messages` request with `max_tokens=1` to validate the API key
- [ ] Both use `ai_health_check_timeout` from config (passed to client or read from settings)
- [ ] Health check failure includes a descriptive reason string logged at ERROR level (e.g., "Model not loaded", "API key invalid", "Connection refused")
- [ ] Existing tests pass; new tests for health check scenarios added

## Technical Details

### clients.py — OllamaClient.health_check() (~line 190)
Current: Checks `/api/tags` only (connectivity test).
Change: After `/api/tags` succeeds, send a minimal generate request:
```python
async def health_check(self) -> bool:
    try:
        async with httpx.AsyncClient(timeout=self._health_timeout) as client:
            # Step 1: Check API is reachable
            resp = await client.get(f"{self.base_url}/api/tags")
            resp.raise_for_status()
            # Step 2: Verify model is loaded with minimal completion
            resp = await client.post(
                f"{self.base_url}/api/generate",
                json={"model": self.model, "prompt": "Say OK", "stream": False},
            )
            resp.raise_for_status()
        return True
    except (httpx.ConnectError, httpx.TimeoutException) as e:
        logger.error(f"Ollama health check failed: {e}")
        return False
```

### clients.py — ClaudeClient.health_check() (~line 259)
Current: Checks `https://api.anthropic.com/` (generic connectivity).
Change: Send a minimal messages API call:
```python
async def health_check(self) -> bool:
    try:
        async with httpx.AsyncClient(timeout=self._health_timeout) as client:
            resp = await client.post(
                "https://api.anthropic.com/v1/messages",
                headers={"x-api-key": self.api_key, "anthropic-version": "2023-06-01", "content-type": "application/json"},
                json={"model": self.model, "max_tokens": 1, "messages": [{"role": "user", "content": "hi"}]},
            )
            if resp.status_code == 401:
                logger.error("Claude health check failed: API key invalid")
                return False
            resp.raise_for_status()
        return True
    except (httpx.ConnectError, httpx.TimeoutException) as e:
        logger.error(f"Claude health check failed: {e}")
        return False
```

### Health check timeout
Pass `ai_health_check_timeout` to client constructors or accept it in `health_check()`. The `get_client()` factory can pass it through.

## Dependencies
- [ ] Task 001 (config settings for `ai_health_check_timeout`)

## Effort Estimate
- Size: S
- Hours: 1-2
- Parallel: true (parallel with tasks 3-5 after task 1)

## Definition of Done
- [ ] Code implemented
- [ ] Tests for health check success/failure/timeout scenarios
- [ ] Existing tests pass
