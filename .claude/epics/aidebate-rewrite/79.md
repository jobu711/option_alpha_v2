---
name: Rewrite clients.py with official SDKs
status: open
created: 2026-02-13T14:33:53Z
updated: 2026-02-13T14:36:49Z
github: https://github.com/jobu711/option_alpha_v2/issues/79
depends_on: [78]
parallel: false
conflicts_with: [80, 81]
---

# Task: Rewrite clients.py with official SDKs

## Description

Rewrite `src/option_alpha/ai/clients.py` from scratch to use `anthropic.AsyncAnthropic` and `ollama.AsyncClient` instead of raw httpx. This is the critical-path task — all other AI module changes depend on the client layer working correctly.

## Acceptance Criteria

- [ ] `OllamaClient` wraps `ollama.AsyncClient` with `chat()` for completions and `list()` for health check
- [ ] `ClaudeClient` wraps `anthropic.AsyncAnthropic` with `messages.create()` for completions and health check
- [ ] `LLMClient` ABC preserved with same `complete(messages, response_model)` and `health_check()` signatures
- [ ] `get_client(config)` factory function preserved with same signature and behavior
- [ ] `timeout` attribute preserved on both clients (orchestrator sets `client.timeout = ...`)
- [ ] Structured output: Ollama uses `format="json"` + `_build_example_hint()`, Claude uses tool-use or JSON mode
- [ ] No raw httpx calls for LLM communication
- [ ] No `_extract_json_from_text()` or `_parse_structured_output()` functions
- [ ] `_build_example_hint()` retained for Ollama JSON guidance

## Technical Details

### OllamaClient
```python
class OllamaClient(LLMClient):
    def __init__(self, model, base_url, timeout, health_check_timeout):
        self._client = ollama.AsyncClient(host=base_url, timeout=timeout)
        # ...

    async def complete(self, messages, response_model=None):
        # Use self._client.chat(model=..., messages=..., format="json")
        # Parse response.message.content as JSON -> response_model.model_validate()

    async def health_check(self):
        # Use self._client.list() to check API reachable + model present
```

### ClaudeClient
```python
class ClaudeClient(LLMClient):
    def __init__(self, api_key, model, timeout, health_check_timeout):
        self._client = anthropic.AsyncAnthropic(api_key=api_key, timeout=timeout)
        # ...

    async def complete(self, messages, response_model=None):
        # Separate system message from conversation (same as current)
        # Use self._client.messages.create(model=..., messages=..., max_tokens=2048)
        # If response_model: parse JSON from response text -> model_validate()

    async def health_check(self):
        # Minimal messages.create() with max_tokens=1
```

### Key interface contracts (from orchestrator.py)
- `get_client(self.settings)` returns `LLMClient`
- Orchestrator sets `client.timeout = effective["ai_request_timeout"]` after creation
- Health check called as `await client.health_check()`

### Files affected
- `src/option_alpha/ai/clients.py` — full rewrite

## Dependencies

- [ ] Task 78 (SDK dependencies installed)

## Effort Estimate

- Size: L
- Hours: 4
- Parallel: false (critical path)

## Definition of Done

- [ ] clients.py rewritten with SDK clients
- [ ] LLMClient ABC interface preserved
- [ ] get_client() factory preserved
- [ ] No raw httpx imports in clients.py
