---
name: Performance tuning for 3k-scale universe
status: open
created: 2026-02-11T14:35:48Z
updated: 2026-02-11T14:41:55Z
github: https://github.com/jobu711/option_alpha_v2/issues/25
depends_on: [22]
parallel: true
conflicts_with: []
---

# Task: Performance tuning for 3k-scale universe

## Description

Tune the existing data fetching infrastructure to handle ~3,000 tickers within the 15-minute scan time target. Increase default batch size and worker count, validate performance at scale, and add parallel cache loading if needed. The existing Parquet cache (18-hour TTL) and failure cache (24-hour TTL) should already handle most of the load after the first scan — this task validates that assumption and makes targeted adjustments.

## Acceptance Criteria

- [ ] Default `fetch_batch_size` increased from 20 to 50
- [ ] Default `fetch_max_workers` increased from 2 to 4
- [ ] Full universe scan (~3,000 tickers) completes within 15 minutes with warm cache
- [ ] First-time full scan (cold cache) completes within 30 minutes (acceptable for initial run)
- [ ] Cache hit rate logged at scan start (e.g., "Cache: 2,850/3,000 tickers cached, 150 to fetch")
- [ ] Stale cache eviction: remove Parquet files for tickers no longer in universe (after 30 days)
- [ ] No increase in yfinance rate-limit errors at higher batch sizes

## Technical Details

**Files to modify:**
- `src/option_alpha/config.py` — Update `fetch_batch_size` default to 50, `fetch_max_workers` default to 4
- `src/option_alpha/data/cache.py` — Add `evict_stale_cache(universe_tickers, max_age_days=30)` function
- `src/option_alpha/pipeline/orchestrator.py` — Add cache stats logging in Phase 1

**Performance model:**
- 3,000 tickers with 95% cache hit = 150 tickers to fetch
- 150 tickers / 50 per batch = 3 batches
- 3 batches with 4 workers = ~1 parallel round
- Each batch ~10-15 seconds (yfinance download)
- Total fetch time: ~15-30 seconds for warm cache
- Scoring + catalysts + options + AI: ~10-12 minutes (unchanged)
- **Total: well under 15 minutes with warm cache**

**Cold cache scenario:**
- 3,000 tickers / 50 per batch = 60 batches
- 60 batches / 4 workers = 15 rounds
- Each round ~15 seconds = ~225 seconds (~4 minutes) for data fetch
- With stagger delays: ~6-8 minutes
- Scoring + downstream: ~15-20 minutes
- **Total: ~25 minutes (acceptable for first run only)**

**Cache eviction:**
```python
def evict_stale_cache(current_universe: set[str], max_age_days: int = 30) -> int:
    """Remove cached Parquet files for tickers no longer in universe."""
    ...
```

## Dependencies

- [ ] Task 005 (pipeline integration) — orchestrator must use dynamic universe
- [ ] Existing cache infrastructure (cache.py, fetcher.py) — no changes to core logic

## Effort Estimate

- Size: S
- Hours: 3-4
- Parallel: true (can run alongside Task 6)

## Definition of Done

- [ ] Default batch/worker settings increased
- [ ] Full scan with warm cache under 15 minutes (manual benchmark)
- [ ] Cache stats logged at scan start
- [ ] Stale cache eviction function works
- [ ] No rate-limit regressions at higher batch sizes
