---
name: AI multi-agent debate system
status: open
created: 2026-02-11T00:53:50Z
updated: 2026-02-11T00:55:00Z
github: https://github.com/jobu711/option_alpha_v2/issues/11
depends_on: [7, 10]
parallel: false
conflicts_with: []
---

# Task: AI multi-agent debate system

## Description
Build the multi-agent LLM debate system: Ollama and Claude API client abstraction, Bull/Bear/Risk agent implementations with structured Pydantic output via instructor library, context builder that curates ~2000 token prompts from scoring and options data, and the debate pipeline that processes the top 10 candidates. Includes retry logic and conservative fallback defaults.

## Acceptance Criteria
- [ ] `ai/clients.py` provides unified LLM client abstraction supporting Ollama (local) and Claude API (cloud)
- [ ] Ollama client: connection verification, model selection, httpx-based async calls
- [ ] Claude client: Anthropic API via httpx, API key from env/config
- [ ] `ai/agents.py` implements Bull agent (bullish thesis), Bear agent (counter-arguments), Risk agent (synthesis + conviction 1-10 + trade/no-trade)
- [ ] `ai/context.py` builds ~2000 token context from scoring breakdown + options data per ticker
- [ ] `ai/debate.py` orchestrates sequential debate: Bull -> Bear (receives bull thesis) -> Risk (receives both)
- [ ] Structured output via instructor library with Pydantic models: TradeThesis, AgentResponse, DebateResult
- [ ] 3 retries on parse failure with plain-text fallback
- [ ] Conservative defaults on total failure: no_trade, conviction=3
- [ ] Processes top 10 candidates from scoring pipeline
- [ ] Unit tests with mocked LLM responses

## Technical Details
- instructor library: `instructor.from_openai()` for Ollama (OpenAI-compatible), `instructor.from_anthropic()` for Claude
- Pydantic models: `TradeThesis(direction, conviction, entry_rationale, risk_factors, recommended_action)`, `AgentResponse(role, analysis, key_points)`, `DebateResult(bull, bear, risk, final_thesis)`
- Context builder: format scoring breakdown + options recommendation into concise structured text
- Ollama default model: `llama3.1:8b` or configurable
- Sequential debate ensures Bear sees Bull's thesis, Risk sees both
- Async execution with `asyncio` for non-blocking API calls

## Dependencies
- [ ] Task 002 (scoring data for context building)
- [ ] Task 003 (options data for context building)

## Effort Estimate
- Size: L
- Hours: 8-10
- Parallel: false (depends on Tasks 2+3)

## Definition of Done
- [ ] Code implemented
- [ ] Tests written and passing
- [ ] Debate produces structured output for mocked data
